import time
import numpy as np
import random
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()


"""
=================== Load Data ====================

æ•°æ®é›†ä½¿ç”¨çš„æ˜¯æ¥è‡ªMatt Mahoneyçš„ç»´åŸºç™¾ç§‘æ–‡ç« ï¼Œæ•°æ®é›†å·²ç»è¢«æ¸…æ´—è¿‡ï¼Œå»é™¤äº†ç‰¹æ®Šç¬¦å·ç­‰ï¼Œå¹¶ä¸æ˜¯å…¨é‡æ•°æ®ï¼Œåªæ˜¯éƒ¨åˆ†æ•°æ®ï¼Œæ‰€ä»¥å®é™…ä¸Šæœ€åè®­ç»ƒå‡ºçš„ç»“æœå¾ˆä¸€èˆ¬ï¼ˆè¯­æ–™ä¸å¤Ÿï¼‰ã€‚

å¦‚æœæƒ³è·å–æ›´å…¨çš„è¯­æ–™æ•°æ®ï¼Œå¯ä»¥è®¿é—®ä»¥ä¸‹ç½‘ç«™ï¼Œè¿™æ˜¯gensimä¸­Word2Vecæä¾›çš„è¯­æ–™ï¼š
æ¥è‡ªMatt Mahoneyé¢„å¤„ç†åçš„æ–‡æœ¬å­é›†(http://mattmahoney.net/dc/enwik9.zip)ï¼Œé‡Œé¢åŒ…å«äº†10äº¿ä¸ªå­—ç¬¦ã€‚
ä¸ç¬¬ä¸€æ¡ä¸€æ ·çš„ç»è¿‡é¢„å¤„ç†çš„æ–‡æœ¬æ•°æ®(http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2)ï¼Œä½†æ˜¯åŒ…å«äº†30ä¸ªäº¿çš„å­—ç¬¦ã€‚
å¤šç§è¯­è¨€çš„è®­ç»ƒæ–‡æœ¬(http://www.statmt.org/wmt11/translation-task.html#download)ã€‚
UMBC webbase corpus(http://ebiquity.umbc.edu/redirect/to/resource/id/351/UMBC-webbase-corpus)
"""

with open('data/en_text8') as f:
    text = f.read()

"""
=================== æ•°æ®é¢„å¤„ç† ====================

æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ä¸»è¦åŒ…æ‹¬ï¼š
1.æ›¿æ¢æ–‡æœ¬ä¸­ç‰¹æ®Šç¬¦å·å¹¶å»é™¤ä½é¢‘è¯
2.å¯¹æ–‡æœ¬åˆ†è¯
3.æ„å»ºè¯­æ–™
4.å•è¯æ˜ å°„è¡¨
"""


# å®šä¹‰å‡½æ•°æ¥å®Œæˆæ•°æ®çš„é¢„å¤„ç†
def preprocess(text_file, freq=5):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†

    :param text_file: æ–‡æœ¬æ•°æ®
    :param freq: è¯é¢‘é˜ˆå€¼
    :return:
    """

    # å¯¹æ–‡æœ¬ä¸­çš„ç¬¦å·è¿›è¡Œæ›¿æ¢
    text_file = text_file.lower()
    text_file = text_file.replace('.', ' <PERIOD> ')
    text_file = text_file.replace(',', ' <COMMA> ')
    text_file = text_file.replace('"', ' <QUOTATION_MARK> ')
    text_file = text_file.replace(';', ' <SEMICOLON> ')
    text_file = text_file.replace('!', ' <EXCLAMATION_MARK> ')
    text_file = text_file.replace('?', ' <QUESTION_MARK> ')
    text_file = text_file.replace('(', ' <LEFT_PAREN> ')
    text_file = text_file.replace(')', ' <RIGHT_PAREN> ')
    text_file = text_file.replace('--', ' <HYPHENS> ')
    text_file = text_file.replace('?', ' <QUESTION_MARK> ')
    # text = text.replace('\n', ' <NEW_LINE> ')
    text_file = text_file.replace(':', ' <COLON> ')
    words_text = text_file.split()

    # åˆ é™¤ä½é¢‘è¯ï¼Œå‡å°‘å™ªéŸ³å½±å“
    word_counts = Counter(words_text)
    trimmed_words = [word for word in words_text if word_counts[word] > freq]

    return trimmed_words


# æ¸…æ´—æ–‡æœ¬å¹¶åˆ†è¯
words = preprocess(text)
print(words[:20])

# æ„å»ºæ˜ å°„è¡¨
vocab = set(words)
vocab_to_int = {w: i for i, w in enumerate(vocab)}
int_to_vocab = {i: w for i, w in enumerate(vocab)}

print(type(vocab))

print("total words: {}".format(len(words)))
print("unique words: {}".format(len(set(words))))

# å¯¹åŸæ–‡æœ¬è¿›è¡Œvocabåˆ°intçš„è½¬æ¢
int_words = [vocab_to_int[w] for w in words]


"""
=================== é‡‡æ · ====================

å¯¹åœç”¨è¯è¿›è¡Œé‡‡æ ·ï¼Œä¾‹å¦‚â€œtheâ€ï¼Œ â€œofâ€ä»¥åŠâ€œforâ€è¿™ç±»å•è¯è¿›è¡Œå‰”é™¤ã€‚å‰”é™¤è¿™äº›å•è¯ä»¥åèƒ½å¤ŸåŠ å¿«æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å™ªéŸ³ã€‚

é‡‡ç”¨ä»¥ä¸‹å…¬å¼:
$$ P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}} $$

å…¶ä¸­$ t $æ˜¯ä¸€ä¸ªé˜ˆå€¼å‚æ•°ï¼Œä¸€èˆ¬ä¸º1e-3è‡³1e-5ã€‚  
$f(w_i)$ æ˜¯å•è¯ $w_i$ åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­çš„å‡ºç°é¢‘æ¬¡ã€‚  
$P(w_i)$ æ˜¯å•è¯è¢«åˆ é™¤çš„æ¦‚ç‡ã€‚

"""
# tå€¼ï¼šé˜ˆå€¼å‚æ•°ï¼Œä¸€èˆ¬ä¸º1e-3è‡³1e-5
t = 1e-5
# å‰”é™¤æ¦‚ç‡é˜ˆå€¼
threshold = 0.8

# ç»Ÿè®¡å•è¯å‡ºç°é¢‘æ¬¡
int_word_counts = Counter(int_words)
total_count = len(int_words)
# è®¡ç®—å•è¯é¢‘ç‡
word_freq = {w: c/total_count for w, c in int_word_counts.items()}
# è®¡ç®—è¢«åˆ é™¤çš„æ¦‚ç‡
prob_drop = {w: 1 - np.sqrt(t / word_freq[w]) for w in int_word_counts}

"""====== åŸè®ºæ–‡å…¬å¼ ======"""
# ä¿ç•™çš„æ¦‚ç‡å…¬å¼
# prob_reserve = {w: (np.sqrt(word_freq[w] / 0.001) + 1) * (0.001 / word_freq[w]) for w in int_word_counts}
# prob_drop = 1 - prob_reserve

# å¯¹å•è¯è¿›è¡Œé‡‡æ ·
train_words = [w for w in int_words if prob_drop[w] < threshold]

print(len(train_words))


"""
=================== æ„é€ batch ====================

Skip-Gramæ¨¡å‹æ˜¯é€šè¿‡è¾“å…¥è¯æ¥é¢„æµ‹ä¸Šä¸‹æ–‡ã€‚å› æ­¤æˆ‘ä»¬è¦æ„é€ æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬ã€‚

å¯¹äºä¸€ä¸ªç»™å®šè¯ï¼Œç¦»å®ƒè¶Šè¿‘çš„è¯å¯èƒ½ä¸å®ƒè¶Šç›¸å…³ï¼Œç¦»å®ƒè¶Šè¿œçš„è¯è¶Šä¸ç›¸å…³ï¼Œè¿™é‡Œè®¾ç½®çª—å£å¤§å°ä¸º5ï¼Œå¯¹äºæ¯ä¸ªè®­ç»ƒå•è¯ï¼Œ
æˆ‘ä»¬è¿˜ä¼šåœ¨[1:5]ä¹‹é—´éšæœºç”Ÿæˆä¸€ä¸ªæ•´æ•°Rï¼Œç”¨Rä½œä¸ºæœ€ç»ˆé€‰æ‹©output wordçš„çª—å£å¤§å°ã€‚è¿™é‡Œä¹‹æ‰€ä»¥å¤šåŠ äº†ä¸€æ­¥éšæœºæ•°çš„çª—å£é‡æ–°é€‰æ‹©æ­¥éª¤ï¼Œ
æ˜¯ä¸ºäº†èƒ½å¤Ÿè®©æ¨¡å‹æ›´èšç„¦äºå½“å‰input wordçš„é‚»è¿‘è¯ã€‚
"""


def get_targets(words_text, idx, window_size=5):
    """
    è·å¾—input wordçš„ä¸Šä¸‹æ–‡å•è¯åˆ—è¡¨

    :param words_text: å•è¯åˆ—è¡¨
    :param idx: input wordçš„ç´¢å¼•å·
    :param window_size: çª—å£å¤§å°
    :return: input wordçš„ä¸Šä¸‹æ–‡å•è¯åˆ—è¡¨
    """
    target_window = np.random.randint(1, window_size + 1)
    # è¿™é‡Œè¦è€ƒè™‘input wordå‰é¢å•è¯ä¸å¤Ÿçš„æƒ…å†µ
    start_point = idx - target_window if (idx - target_window) > 0 else 0
    end_point = idx + target_window
    # output words(å³çª—å£ä¸­çš„ä¸Šä¸‹æ–‡å•è¯)
    targets = set(words_text[start_point: idx] + words_text[idx + 1: end_point + 1])
    return list(targets)


def get_batches(words_text, batch_size, window_size=5):
    """
    æ„é€ ä¸€ä¸ªè·å–batchçš„ç”Ÿæˆå™¨

    :param words_text:
    :param batch_size:
    :param window_size:
    :return:
    """
    n_batches = len(words_text) // batch_size

    # ä»…å–full batches
    words_text = words_text[:n_batches * batch_size]

    for idx in range(0, len(words_text), batch_size):
        x, y = [], []
        batch = words_text[idx: idx + batch_size]
        for i in range(len(batch)):
            batch_x = batch[i]
            batch_y = get_targets(batch, i, window_size)
            # ç”±äºä¸€ä¸ªinput wordä¼šå¯¹åº”å¤šä¸ªoutput wordï¼Œå› æ­¤éœ€è¦é•¿åº¦ç»Ÿä¸€
            x.extend([batch_x] * len(batch_y))
            y.extend(batch_y)
        yield x, y


"""
=================== æ„å»ºç½‘ç»œ ====================

è¯¥éƒ¨åˆ†ä¸»è¦åŒ…æ‹¬ï¼š
1.è¾“å…¥å±‚
2.Embedding
3.Negative Sampling
"""
# Inputs Layer
train_graph = tf.Graph()
with train_graph.as_default():
    inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')
    labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')

# Embedding Layer
# åµŒå…¥çŸ©é˜µçš„çŸ©é˜µå½¢çŠ¶ä¸º  ğ‘£ğ‘œğ‘ğ‘ğ‘_ğ‘ ğ‘–ğ‘§ğ‘’Ã—â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘¢ğ‘›ğ‘–ğ‘¡ğ‘ _ğ‘ ğ‘–ğ‘§ğ‘’
# TensorFlowä¸­çš„tf.nn.embedding_lookupå‡½æ•°å¯ä»¥å®ç°lookupçš„è®¡ç®—æ–¹å¼
vocab_size = len(int_to_vocab)

# åµŒå…¥ç»´åº¦ ä¸€èˆ¬å–ï¼š50-300
embedding_size = 200

with train_graph.as_default():
    # åµŒå…¥å±‚æƒé‡çŸ©é˜µ
    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))
    # å®ç°lookup
    embed = tf.nn.embedding_lookup(embedding, inputs)

# Negative Sampling
# è´Ÿé‡‡æ ·ä¸»è¦æ˜¯ä¸ºäº†è§£å†³æ¢¯åº¦ä¸‹é™è®¡ç®—é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚
# TensorFlowä¸­çš„tf.nn.sampled_softmax_lossä¼šåœ¨softmaxå±‚ä¸Šè¿›è¡Œé‡‡æ ·è®¡ç®—æŸå¤±ï¼Œè®¡ç®—å‡ºçš„lossè¦æ¯”full softmax lossä½ã€‚
n_sampled = 100

with train_graph.as_default():
    softmax_w = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1))
    softmax_b = tf.Variable(tf.zeros(vocab_size))

    # è®¡ç®—negative samplingä¸‹çš„æŸå¤±
    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)

    cost = tf.reduce_mean(loss)
    optimizer = tf.train.AdamOptimizer().minimize(cost)


"""
=================== éªŒè¯ ====================

ä¸ºäº†æ›´åŠ ç›´è§‚çš„çœ‹åˆ°è®­ç»ƒçš„ç»“æœï¼Œå°†æŸ¥çœ‹è®­ç»ƒå‡ºçš„ç›¸è¿‘è¯­ä¹‰çš„è¯ã€‚
"""
with train_graph.as_default():
    # éšæœºæŒ‘é€‰ä¸€äº›å•è¯
    valid_size = 16
    valid_window = 100
    # ä»ä¸åŒä½ç½®å„é€‰8ä¸ªå•è¯
    valid_examples = np.array(random.sample(range(valid_window), valid_size // 2))
    valid_examples = np.append(valid_examples,
                               random.sample(range(1000, 1000 + valid_window), valid_size // 2))

    valid_size = len(valid_examples)
    # éªŒè¯å•è¯é›†
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # è®¡ç®—æ¯ä¸ªè¯å‘é‡çš„æ¨¡å¹¶è¿›è¡Œå•ä½åŒ–
    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))
    normalized_embedding = embedding / norm
    # æŸ¥æ‰¾éªŒè¯å•è¯çš„è¯å‘é‡
    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))


epochs = 10  # è¿­ä»£è½®æ•°
batch_size = 1000  # batchå¤§å°
window_size = 10  # çª—å£å¤§å°

with train_graph.as_default():
    saver = tf.train.Saver()  # æ–‡ä»¶å­˜å‚¨

with tf.Session(graph=train_graph) as sess:
    iteration = 1
    loss = 0
    sess.run(tf.global_variables_initializer())

    for e in range(1, epochs + 1):
        batches = get_batches(train_words, batch_size, window_size)
        start = time.time()
        #
        for x, y in batches:

            feed = {inputs: x,
                    labels: np.array(y)[:, None]}
            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)

            loss += train_loss

            if iteration % 100 == 0:
                end = time.time()
                print("Epoch {}/{}".format(e, epochs),
                      "Iteration: {}".format(iteration),
                      "Avg. Training loss: {:.4f}".format(loss / 100),
                      "{:.4f} sec/batch".format((end - start) / 100))
                loss = 0
                start = time.time()

            # è®¡ç®—ç›¸ä¼¼çš„è¯
            if iteration % 1000 == 0:
                # è®¡ç®—similarity
                sim = similarity.eval()
                for i in range(valid_size):
                    valid_word = int_to_vocab[valid_examples[i]]
                    top_k = 8  # å–æœ€ç›¸ä¼¼å•è¯çš„å‰8ä¸ª
                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]
                    log = 'Nearest to [%s]:' % valid_word
                    for k in range(top_k):
                        close_word = int_to_vocab[nearest[k]]
                        log = '%s %s,' % (log, close_word)
                    print(log)

            iteration += 1

    save_path = saver.save(sess, "checkpoints/en_text8.ckpt")
    embed_mat = sess.run(normalized_embedding)


viz_words = 500
tsne = TSNE()
embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])

fig, ax = plt.subplots(figsize=(14, 14))
for idx in range(viz_words):
    plt.scatter(*embed_tsne[idx, :], color='steelblue')
    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)
